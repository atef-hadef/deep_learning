# training/train_sentiment_roberta.py
"""
Script de fine-tuning du mod√®le RoBERTa pour l'analyse de sentiments.
Utilise le dataset tweet_eval (cardiffnlp) pour affiner le mod√®le de base.

Labels:
  0 -> negative
  1 -> neutral
  2 -> positive
"""

import os
import numpy as np
from datetime import datetime

from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)
from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support

# ===========================
# Configuration de base
# ===========================
BASE_MODEL_NAME = "cardiffnlp/twitter-roberta-base-sentiment-latest"
OUTPUT_DIR = "./models/custom-roberta-sentiment"  # dossier final du mod√®le

# üîß Adapt√© pour petit GPU (MX350)
MAX_SEQ_LENGTH = 96          # Optimal pour tweets (plus rapide, moins de VRAM)
NUM_EPOCHS = 1               # Commencer avec 1 epoch pour test GPU
PER_DEVICE_BATCH_SIZE = 2    # Tr√®s petit batch par GPU (limite VRAM)
GRAD_ACC_STEPS = 8           # 2 * 8 = batch effectif de 16
LR = 2e-5

# Optionnel : r√©duire taille dataset pour tests rapides
USE_SUBSET = False           # False = dataset complet (45K tweets) - RECOMMAND√â pour GPU
TRAIN_SUBSET_SIZE = 10000    # Utilis√© seulement si USE_SUBSET=True
EVAL_SUBSET_SIZE = 2000      # Utilis√© seulement si USE_SUBSET=True


def load_tweet_eval_sentiment():
    """
    Charge le dataset 'tweet_eval' (task 'sentiment').

    Returns:
        DatasetDict contenant train, validation, test

    Labels:
      0 -> negative
      1 -> neutral
      2 -> positive
    """
    print("üîπ Chargement du dataset tweet_eval/sentiment ...")
    dataset = load_dataset("cardiffnlp/tweet_eval", "sentiment")

    print(f"   - Train samples: {len(dataset['train'])}")
    print(f"   - Validation samples: {len(dataset['validation'])}")
    print(f"   - Test samples: {len(dataset['test'])}")

    return dataset


def preprocess_dataset(dataset, tokenizer):
    """
    Tokenisation du texte pour RoBERTa.

    Args:
        dataset: Dataset Hugging Face
        tokenizer: Tokenizer RoBERTa

    Returns:
        Dataset tokenis√© et format√© pour PyTorch
    """
    print("üîπ Tokenisation du dataset ...")

    def tokenize_batch(batch):
        return tokenizer(
            batch["text"],
            truncation=True,
            padding="max_length",
            max_length=MAX_SEQ_LENGTH,
        )

    tokenized = dataset.map(tokenize_batch, batched=True)

    # Hugging Face Trainer attend ces colonnes :
    tokenized = tokenized.rename_column("label", "labels")
    tokenized.set_format(
        type="torch",
        columns=["input_ids", "attention_mask", "labels"],
    )

    print("‚úÖ Tokenisation termin√©e")
    return tokenized


def compute_metrics(eval_pred):
    """
    M√©triques pour validation/test : accuracy + F1 macro.

    Args:
        eval_pred: Tuple (logits, labels)

    Returns:
        Dict avec accuracy, f1_macro, precision_macro, recall_macro
    """
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)

    acc = accuracy_score(labels, preds)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, preds, average="macro", zero_division=0
    )

    return {
        "accuracy": acc,
        "f1_macro": f1,
        "precision_macro": precision,
        "recall_macro": recall,
    }


def save_training_info(output_dir, metrics):
    """
    Sauvegarde informations sur l'entra√Ænement dans un fichier texte.

    Args:
        output_dir: R√©pertoire de sortie
        metrics: Dictionnaire des m√©triques finales
    """
    info_file = os.path.join(output_dir, "training_info.txt")

    with open(info_file, "w", encoding="utf-8") as f:
        f.write("=" * 60 + "\n")
        f.write("INFORMATIONS SUR L'ENTRA√éNEMENT\n")
        f.write("=" * 60 + "\n\n")

        f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Mod√®le de base: {BASE_MODEL_NAME}\n")
        f.write(f"Dataset: tweet_eval/sentiment\n\n")

        f.write("Hyperparam√®tres:\n")
        f.write(f"  - Max sequence length: {MAX_SEQ_LENGTH}\n")
        f.write(f"  - Epochs: {NUM_EPOCHS}\n")
        f.write(f"  - Per-device batch size: {PER_DEVICE_BATCH_SIZE}\n")
        f.write(f"  - Gradient accumulation steps: {GRAD_ACC_STEPS}\n")
        f.write(f"  - Learning rate: {LR}\n")
        f.write(f"  - Weight decay: 0.01\n\n")

        if metrics:
            f.write("M√©triques finales (test set):\n")
            for key, value in metrics.items():
                f.write(f"  - {key}: {value:.4f}\n")

        f.write("\n" + "=" * 60 + "\n")

    print(f"üìÑ Informations d'entra√Ænement sauvegard√©es dans {info_file}")


def count_trainable_parameters(model):
    """
    Retourne le nombre de param√®tres entra√Ænables (pour info logs).
    """
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def main():
    """
    Fonction principale d'entra√Ænement.
    """
    print("=" * 80)
    print("üöÄ FINE-TUNING ROBERTA POUR ANALYSE DE SENTIMENTS")
    print("=" * 80 + "\n")

    # 1) Charger dataset
    raw_dataset = load_tweet_eval_sentiment()

    # 2) Charger tokenizer + mod√®le RoBERTa existant
    print(f"\nüîπ Chargement du mod√®le de base : {BASE_MODEL_NAME}")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)
    model = AutoModelForSequenceClassification.from_pretrained(
        BASE_MODEL_NAME,
        num_labels=3,  # n√©gatif / neutre / positif
    )
    print("‚úÖ Mod√®le et tokenizer charg√©s")

    # üîß IMPORTANT : geler le backbone RoBERTa pour √©conomiser la VRAM
    print("\nüßä Gel du backbone RoBERTa (on entra√Æne seulement la t√™te de classification)...")
    for param in model.roberta.parameters():
        param.requires_grad = False

    trainable_params = count_trainable_parameters(model)
    print(f"   ‚Üí Param√®tres entra√Ænables: {trainable_params:,}")

    # 3) Pr√©traitement / tokenisation
    print("\nüîπ Pr√©traitement des donn√©es ...")
    tokenized_dataset = preprocess_dataset(raw_dataset, tokenizer)

    train_dataset = tokenized_dataset["train"]
    eval_dataset = tokenized_dataset["validation"]

    # (Optionnel) : r√©duire un peu la taille pour un test rapide
    if USE_SUBSET:
        print(f"\n‚ö†Ô∏è  Mode SUBSET activ√© - R√©duction dataset pour test rapide")
        train_dataset = train_dataset.select(range(min(TRAIN_SUBSET_SIZE, len(train_dataset))))
        eval_dataset = eval_dataset.select(range(min(EVAL_SUBSET_SIZE, len(eval_dataset))))
        print(f"   - Train size: {len(train_dataset)}")
        print(f"   - Eval size: {len(eval_dataset)}")

    # 4) Config d'entra√Ænement
    print(f"\nüîπ Configuration de l'entra√Ænement ...")
    print(f"   - Output directory: {OUTPUT_DIR}")
    print(f"   - Epochs: {NUM_EPOCHS}")
    print(f"   - Per-device batch size: {PER_DEVICE_BATCH_SIZE}")
    print(f"   - Gradient accumulation steps: {GRAD_ACC_STEPS}")
    print(f"   - Max sequence length: {MAX_SEQ_LENGTH}")
    print(f"   - Learning rate: {LR}")
    print(f"   - Dataset size: {len(train_dataset)} train, {len(eval_dataset)} eval")

    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,
        per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE,
        gradient_accumulation_steps=GRAD_ACC_STEPS,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        learning_rate=LR,
        weight_decay=0.01,
        load_best_model_at_end=True,
        metric_for_best_model="f1_macro",
        logging_steps=200,  # Log moins fr√©quemment (optimis√© pour 1 epoch)
        logging_dir=os.path.join(OUTPUT_DIR, "logs"),
        save_total_limit=2,  # Garder seulement les 2 meilleurs checkpoints
        fp16=False,          # On peut passer √† True si √ßa passe bien, pour encore r√©duire la VRAM
        dataloader_num_workers=0,  # 0 pour √©viter probl√®mes Windows multiprocessing
    )

    # 5) Trainer HF
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    # 6) Fine-tuning
    print("\n" + "=" * 80)
    print("üöÄ LANCEMENT DU FINE-TUNING")
    print("=" * 80 + "\n")

    trainer.train()

    print("\n‚úÖ Entra√Ænement termin√©. Sauvegarde du mod√®le ...")
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print(f"‚úÖ Mod√®le sauvegard√© dans {OUTPUT_DIR}")

    # 7) √âvaluation finale sur le test set
    print("\n" + "=" * 80)
    print("üìä √âVALUATION SUR LE JEU DE TEST")
    print("=" * 80 + "\n")

    test_dataset = tokenized_dataset["test"]
    metrics = trainer.evaluate(test_dataset)

    print("\nüìä M√©triques finales (test set):")
    print("-" * 60)
    for key, value in metrics.items():
        if isinstance(value, (int, float)):
            print(f"  {key:.<40} {value:.4f}")
    print("-" * 60)

    # 8) Sauvegarder info d'entra√Ænement
    save_training_info(OUTPUT_DIR, metrics)

    print("\n" + "=" * 80)
    print("‚úÖ FINE-TUNING TERMIN√â AVEC SUCC√àS")
    print("=" * 80)
    print(f"\nüìÅ Mod√®le disponible dans: {OUTPUT_DIR}")
    print(f"üìä Logs TensorBoard disponibles dans: {os.path.join(OUTPUT_DIR, 'logs')}")
    print("\nPour utiliser le mod√®le fine-tun√© dans l'application:")
    print(f"  1. Modifier app/config.py")
    print(f"  2. Changer SENTIMENT_MODEL = '{OUTPUT_DIR}'")
    print(f"  3. Red√©marrer l'application\n")


if __name__ == "__main__":
    main()
