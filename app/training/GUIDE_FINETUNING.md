# Guide Complet - Fine-tuning RoBERTa

Ce guide d√©taill√© vous accompagne pas √† pas dans le processus de fine-tuning du mod√®le RoBERTa pour am√©liorer les performances d'analyse de sentiments.

---

## üìã Table des Mati√®res

1. [Pourquoi Fine-tuner ?](#pourquoi-fine-tuner)
2. [Pr√©requis](#pr√©requis)
3. [Installation](#installation)
4. [√âtape 1 : Pr√©parer l'environnement](#√©tape-1--pr√©parer-lenvironnement)
5. [√âtape 2 : Lancer le fine-tuning](#√©tape-2--lancer-le-fine-tuning)
6. [√âtape 3 : Surveiller l'entra√Ænement](#√©tape-3--surveiller-lentra√Ænement)
7. [√âtape 4 : Tester le mod√®le](#√©tape-4--tester-le-mod√®le)
8. [√âtape 5 : Int√©grer le mod√®le](#√©tape-5--int√©grer-le-mod√®le)
9. [Optimisation et Tuning](#optimisation-et-tuning)
10. [Troubleshooting](#troubleshooting)

---

## Pourquoi Fine-tuner ?

### Avantages du Fine-tuning

‚úÖ **Performance am√©lior√©e** : Adaptation au vocabulaire sp√©cifique de votre domaine  
‚úÖ **Meilleure pr√©cision** : R√©duction des erreurs sur vos cas d'usage  
‚úÖ **Coh√©rence** : Comportement plus pr√©visible et stable  
‚úÖ **Personnalisation** : Adaptation aux nuances de vos donn√©es  

### Quand Fine-tuner ?

- Votre domaine a un vocabulaire sp√©cifique (produits tech, gaming, etc.)
- Le mod√®le de base fait des erreurs r√©currentes
- Vous avez acc√®s √† des donn√©es annot√©es de qualit√©
- Vous voulez maximiser les performances

---

## Pr√©requis

### Mat√©riel Recommand√©

| Configuration | CPU | GPU | Dur√©e Estim√©e |
|--------------|-----|-----|---------------|
| **Minimale** | 4 cores | - | ~4-5 heures |
| **Recommand√©e** | 8 cores | GTX 1060 (6GB) | ~45 min |
| **Optimale** | 16+ cores | RTX 3060+ (12GB) | ~20-30 min |

### Espace Disque

- Dataset : ~500 MB
- Mod√®le base : ~500 MB
- Mod√®le fine-tun√© : ~500 MB
- Checkpoints temporaires : ~1-2 GB
- **Total recommand√© : 5 GB libres**

### Software

- Python 3.9+
- PyTorch 2.0+
- CUDA 11.8+ (si GPU disponible)
- transformers 4.35+

---

## Installation

### 1. Installer les d√©pendances

```bash
cd projet_deep_learning

# Installer les packages de base (d√©j√† fait normalement)
pip install -r requirements.txt

# V√©rifier que datasets est install√©
pip install datasets accelerate
```

### 2. V√©rifier CUDA (GPU)

```bash
python -c "import torch; print(f'CUDA disponible: {torch.cuda.is_available()}'); print(f'Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')"
```

**Sortie attendue :**
```
CUDA disponible: True
Device: NVIDIA GeForce RTX 3060
```

Si `False`, l'entra√Ænement utilisera le CPU (plus lent mais fonctionnel).

### 3. T√©l√©charger spaCy model (si pas d√©j√† fait)

```bash
python -m spacy download en_core_web_sm
```

---

## √âtape 1 : Pr√©parer l'environnement

### 1.1 V√©rifier la structure

```bash
# Votre arborescence devrait ressembler √† :
projet_deep_learning/
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ train_sentiment_roberta.py
‚îÇ   ‚îú‚îÄ‚îÄ test_model.py
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ (vide pour l'instant)
‚îú‚îÄ‚îÄ app/
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ ...
```

### 1.2 Cr√©er le dossier models (si n√©cessaire)

```bash
mkdir -p models
```

### 1.3 Test de connexion Hugging Face

```bash
python -c "from datasets import load_dataset; print('‚úÖ Connection OK')"
```

---

## √âtape 2 : Lancer le fine-tuning

### Mode 1 : Entra√Ænement Complet (Production)

```bash
cd projet_deep_learning
python -m training.train_sentiment_roberta
```

**Ce qui va se passer :**

1. ‚è¨ T√©l√©chargement du dataset tweet_eval (~500 MB) - **1√®re fois uniquement**
2. ‚è¨ Chargement du mod√®le RoBERTa de base (~500 MB) - **1√®re fois uniquement**
3. üîÑ Tokenisation des 45,615 tweets d'entra√Ænement (~2-3 min)
4. üöÄ **Entra√Ænement sur 3 √©poques** :
   - Epoch 1/3 : ~15-20 min (GPU) ou ~1h30 (CPU)
   - Epoch 2/3 : ~15-20 min (GPU) ou ~1h30 (CPU)
   - Epoch 3/3 : ~15-20 min (GPU) ou ~1h30 (CPU)
5. üíæ Sauvegarde du meilleur mod√®le
6. üìä √âvaluation sur test set (12,284 tweets)

**Sortie attendue (exemple) :**

```
================================================================================
üöÄ FINE-TUNING ROBERTA POUR ANALYSE DE SENTIMENTS
================================================================================

üîπ Chargement du dataset tweet_eval/sentiment ...
   - Train samples: 45615
   - Validation samples: 2000
   - Test samples: 12284

üîπ Chargement du mod√®le de base : cardiffnlp/twitter-roberta-base-sentiment-latest
‚úÖ Mod√®le et tokenizer charg√©s

üîπ Tokenisation du dataset ...
‚úÖ Tokenisation termin√©e

üîπ Configuration de l'entra√Ænement ...
   - Output directory: ./models/custom-roberta-sentiment
   - Epochs: 3
   - Batch size: 16
   - Learning rate: 2e-05

================================================================================
üöÄ LANCEMENT DU FINE-TUNING
================================================================================

Epoch 1/3:
[====================] 2851/2851 [15:23<00:00, 3.08it/s]
{'loss': 0.4123, 'learning_rate': 1.5e-05, 'epoch': 1.0}
{'eval_loss': 0.3892, 'eval_accuracy': 0.7150, 'eval_f1_macro': 0.6923}

Epoch 2/3:
[====================] 2851/2851 [15:21<00:00, 3.09it/s]
{'loss': 0.3456, 'learning_rate': 1e-05, 'epoch': 2.0}
{'eval_loss': 0.3721, 'eval_accuracy': 0.7285, 'eval_f1_macro': 0.7045}

Epoch 3/3:
[====================] 2851/2851 [15:19<00:00, 3.10it/s]
{'loss': 0.3102, 'learning_rate': 5e-06, 'epoch': 3.0}
{'eval_loss': 0.3698, 'eval_accuracy': 0.7310, 'eval_f1_macro': 0.7089}

‚úÖ Entra√Ænement termin√©. Sauvegarde du mod√®le ...
‚úÖ Mod√®le sauvegard√© dans ./models/custom-roberta-sentiment

================================================================================
üìä √âVALUATION SUR LE JEU DE TEST
================================================================================

üìä M√©triques finales (test set):
------------------------------------------------------------
  eval_loss.................................... 0.3645
  eval_accuracy................................ 0.7321
  eval_f1_macro................................ 0.7102
  eval_precision_macro......................... 0.7145
  eval_recall_macro............................ 0.7089
------------------------------------------------------------

‚úÖ FINE-TUNING TERMIN√â AVEC SUCC√àS

üìÅ Mod√®le disponible dans: ./models/custom-roberta-sentiment
```

### Mode 2 : Test Rapide (D√©veloppement)

Pour un test rapide (~15 min sur CPU), modifiez `training/train_sentiment_roberta.py` :

```python
# Ligne 23
USE_SUBSET = True  # ‚Üê Changer False √† True
```

Puis lancez :
```bash
python -m training.train_sentiment_roberta
```

**Utilise seulement :**
- 10,000 tweets pour train (au lieu de 45,615)
- 2,000 tweets pour validation (au lieu de 2,000)

---

## √âtape 3 : Surveiller l'entra√Ænement

### Option 1 : Logs en temps r√©el

Les logs s'affichent automatiquement dans le terminal.

### Option 2 : TensorBoard (Recommand√©)

Dans un **nouveau terminal** :

```bash
cd projet_deep_learning
tensorboard --logdir models/custom-roberta-sentiment/logs
```

Ouvrir dans navigateur : **http://localhost:6006**

**Graphiques disponibles :**
- üìâ Loss (train & validation) par √©poque
- üìà Accuracy, F1-macro √©volution
- ‚è±Ô∏è Learning rate schedule
- üî¢ Gradient norms

### Option 3 : Fichier training_info.txt

Apr√®s entra√Ænement, consulter :
```bash
cat models/custom-roberta-sentiment/training_info.txt
```

---

## √âtape 4 : Tester le mod√®le

### Test Automatique (Comparaison)

```bash
python -m training.test_model
```

**Sortie exemple :**

```
üìä COMPARAISON DES PR√âDICTIONS
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Test #1: I absolutely love this product! Best purchase ever! üòç
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

üìå Mod√®le BASE:
   Sentiment: POSITIVE (confidence: 0.892)
   Scores: Pos=0.892 | Neu=0.085 | Neg=0.023

üéØ Mod√®le FINE-TUNED:
   Sentiment: POSITIVE (confidence: 0.947)
   Scores: Pos=0.947 | Neu=0.042 | Neg=0.011

üìà Am√©lioration de confiance: 0.892 ‚Üí 0.947 (+0.055)
```

### Test sur Texte Personnalis√©

```bash
python -m training.test_model "The camera is great but battery sucks"
```

### Test Interactif Python

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Charger le mod√®le fine-tun√©
tokenizer = AutoTokenizer.from_pretrained("./models/custom-roberta-sentiment")
model = AutoModelForSequenceClassification.from_pretrained("./models/custom-roberta-sentiment")

# Test
text = "Amazing product! Highly recommended!"
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)
probs = torch.nn.functional.softmax(outputs.logits, dim=-1)

print(f"Negative: {probs[0][0]:.3f}")
print(f"Neutral:  {probs[0][1]:.3f}")
print(f"Positive: {probs[0][2]:.3f}")
```

---

## √âtape 5 : Int√©grer le mod√®le

### M√©thode 1 : Modifier config.py (Recommand√©)

√âditer `app/config.py` :

```python
class Settings(BaseSettings):
    # ... autres param√®tres ...
    
    # AVANT:
    # sentiment_model: str = "cardiffnlp/twitter-roberta-base-sentiment-latest"
    
    # APR√àS:
    sentiment_model: str = "./models/custom-roberta-sentiment"
```

### M√©thode 2 : Variable d'environnement

√âditer `.env` :

```bash
# Mod√®le fine-tun√© (local)
SENTIMENT_MODEL=./models/custom-roberta-sentiment

# Ou mod√®le de base (Hugging Face)
# SENTIMENT_MODEL=cardiffnlp/twitter-roberta-base-sentiment-latest
```

### Red√©marrer l'application

```bash
uvicorn app.main:app --reload
```

**V√©rifier dans les logs :**
```
INFO: Chargement du mod√®le de sentiment depuis ./models/custom-roberta-sentiment
INFO: ‚úÖ Mod√®le charg√© avec succ√®s
```

### Tester via API

```bash
curl -X POST http://localhost:8000/api/search \
  -H "Content-Type: application/json" \
  -d '{
    "keyword": "iPhone 15",
    "platforms": ["reddit"],
    "limit": 5
  }'
```

---

## Optimisation et Tuning

### Am√©liorer la Pr√©cision

#### 1. Augmenter le nombre d'√©poques

```python
NUM_EPOCHS = 5  # Au lieu de 3
```

‚ö†Ô∏è Risque d'overfitting si > 5

#### 2. Ajuster le learning rate

```python
# Plus petit = entra√Ænement plus stable mais plus lent
LR = 1e-5  # Au lieu de 2e-5

# Plus grand = plus rapide mais risque d'instabilit√©
LR = 3e-5
```

#### 3. Augmenter la longueur de s√©quence

```python
MAX_SEQ_LENGTH = 256  # Au lieu de 128
```

‚ö†Ô∏è Consomme 2x plus de m√©moire

#### 4. Ajouter vos propres donn√©es

Cr√©er `custom_data.json` :

```json
[
  {"text": "Love the new iPhone camera!", "label": 2},
  {"text": "Battery drains too fast", "label": 0},
  {"text": "It's okay, nothing special", "label": 1}
]
```

Modifier `train_sentiment_roberta.py` pour charger ces donn√©es suppl√©mentaires.

### Acc√©l√©rer l'Entra√Ænement

#### 1. Mixed Precision (FP16)

```python
training_args = TrainingArguments(
    ...
    fp16=True,  # ‚Üê Activer (n√©cessite GPU Volta+ ou Ampere)
)
```

Gain : **~40% plus rapide**, m√™me consommation m√©moire

#### 2. Gradient Accumulation

```python
training_args = TrainingArguments(
    ...
    per_device_train_batch_size=8,  # R√©duire
    gradient_accumulation_steps=2,  # ‚Üê Ajouter
)
```

Simule `batch_size=16` avec moins de m√©moire.

#### 3. Augmenter Batch Size (si m√©moire suffisante)

```python
BATCH_SIZE = 32  # Au lieu de 16
```

Gain : **~20% plus rapide**

---

## Troubleshooting

### ‚ùå Erreur : CUDA out of memory

**Sympt√¥me :**
```
RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB
```

**Solutions :**

1. R√©duire batch size
```python
BATCH_SIZE = 8  # Ou m√™me 4
```

2. R√©duire max_seq_length
```python
MAX_SEQ_LENGTH = 64
```

3. Gradient checkpointing
```python
training_args = TrainingArguments(
    ...
    gradient_checkpointing=True,
)
```

4. Utiliser CPU
```python
training_args = TrainingArguments(
    ...
    no_cuda=True,
)
```

---

### ‚ùå Erreur : ConnectionError downloading dataset

**Sympt√¥me :**
```
ConnectionError: Couldn't reach https://huggingface.co/datasets/...
```

**Solutions :**

1. V√©rifier connexion internet
```bash
ping huggingface.co
```

2. Utiliser proxy (si n√©cessaire)
```bash
export HTTP_PROXY=http://proxy:8080
export HTTPS_PROXY=http://proxy:8080
```

3. T√©l√©charger manuellement
```python
from datasets import load_dataset
dataset = load_dataset("cardiffnlp/tweet_eval", "sentiment", cache_dir="./cache")
```

---

### ‚ùå Accuracy ne s'am√©liore pas

**Sympt√¥me :**
Apr√®s 3 √©poques, accuracy stagne √† ~0.50-0.60

**Diagnostics :**

1. **Dataset d√©s√©quilibr√©** : V√©rifier distribution des labels
```python
from collections import Counter
labels = [ex['label'] for ex in dataset['train']]
print(Counter(labels))
```

2. **Learning rate trop √©lev√©** : R√©duire √† `1e-5`

3. **Underfitting** : Augmenter √©poques √† 5

4. **Overfitting** : V√©rifier `eval_loss` :
   - Si `train_loss` ‚Üì mais `eval_loss` ‚Üë ‚Üí overfitting

---

### ‚ö†Ô∏è Warning : Some weights not initialized

**Sympt√¥me :**
```
Some weights of the model checkpoint at ... were not used when initializing...
```

**Explication :** Normal pour fine-tuning. Le mod√®le adapte ses poids.

**Action :** Aucune (c'est attendu)

---

## M√©triques de R√©f√©rence

### Mod√®le de Base (sans fine-tuning)

| M√©trique | Score |
|----------|-------|
| Accuracy | ~0.695 |
| F1 Macro | ~0.675 |
| Precision | ~0.680 |
| Recall | ~0.670 |

### Mod√®le Fine-tun√© (attendu)

| M√©trique | Score | Am√©lioration |
|----------|-------|--------------|
| Accuracy | ~0.720-0.740 | +3-5% |
| F1 Macro | ~0.700-0.720 | +3-5% |
| Precision | ~0.710-0.730 | +3-5% |
| Recall | ~0.700-0.720 | +3-5% |

**Objectif r√©aliste :** +3-5% d'am√©lioration sur toutes les m√©triques

---

## Ressources

- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [Tweet Eval Dataset](https://huggingface.co/datasets/cardiffnlp/tweet_eval)
- [RoBERTa Paper](https://arxiv.org/abs/1907.11692)
- [Fine-tuning Guide](https://huggingface.co/docs/transformers/training)

---

**Derni√®re mise √† jour :** 16 novembre 2025
